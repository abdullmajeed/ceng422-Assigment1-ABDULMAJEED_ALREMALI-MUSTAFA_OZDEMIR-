# -*- coding: utf-8 -*-
import re, html, unicodedata
import pandas as pd
from pathlib import Path

try:
    from ftfy import fix_text
except Exception:
    def fix_text(s): return s

# Azerbaijani-aware lowercase
def lower_az(s: str) -> str:
    if not isinstance(s, str): return ""
    s = unicodedata.normalize("NFC", s)
    s = s.replace("I", "ƒ±").replace("ƒ∞", "i")
    s = s.lower().replace("iÃá", "i")
    return s

HTML_TAG_RE = re.compile(r"<[^>]+>")
URL_RE      = re.compile(r"(https?://\S+|www\.\S+)", re.IGNORECASE)
EMAIL_RE    = re.compile(r"\b[\w\.-]+@[\w\.-]+\.\w+\b", re.IGNORECASE)
PHONE_RE    = re.compile(r"\+?\d[\d\-\s\(\)]{6,}\d")
USER_RE     = re.compile(r"@\w+")
MULTI_PUNCT = re.compile(r"([!?.,;:])\1{1,}")
MULTI_SPACE = re.compile(r"\s+")
REPEAT_CHARS= re.compile(r"(.)\1{2,}", flags=re.UNICODE)

TOKEN_RE = re.compile(
    r"[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+(?:'[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+)?"
    r"|<NUM>|URL|EMAIL|PHONE|USER|EMO_(?:POS|NEG)"
)

EMO_MAP = {
    "üòç":"EMO_POS","üòä":"EMO_POS","üòÇ":"EMO_POS","üôÇ":"EMO_POS",
    "üò°":"EMO_NEG","ü§¨":"EMO_NEG","üò≠":"EMO_NEG","üò¢":"EMO_NEG"
}
SLANG_MAP = {"salam":"salam","tmn":"tamam","sagol":"saƒüol","cox":"√ßox","yaxsi":"yax≈üƒ±"}
NEGATORS = {"yox","deyil","he√ß","q…ôtiyy…ôn","yoxdur"}

# Domain helpers (paste from Section 6)
import re
NEWS_HINTS = re.compile(r"\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\b", re.I)
SOCIAL_HINTS = re.compile(r"\b(rt)\b|@|#|(?:üòÇ|üòç|üòä|üëç|üëé|üò°|üôÇ)")
REV_HINTS = re.compile(r"\b(azn|manat|qiym…ôt|aldƒ±m|ulduz|√ßox yax≈üƒ±|√ßox pis)\b", re.I)
PRICE_RE = re.compile(r"\b\d+\s*(azn|manat)\b", re.I)
STARS_RE = re.compile(r"\b([1-5])\s*ulduz\b", re.I)
POS_RATE = re.compile(r"\b√ßox yax≈üƒ±\b")
NEG_RATE = re.compile(r"\b√ßox pis\b")

def detect_domain(text: str) -> str:
    s = text.lower()
    if NEWS_HINTS.search(s): return "news"
    if SOCIAL_HINTS.search(s): return "social"
    if REV_HINTS.search(s): return "reviews"
    return "general"

def domain_specific_normalize(cleaned: str, domain: str) -> str:
    s = cleaned
    if domain == "reviews":
        s = PRICE_RE.sub("<PRICE>", s)
        s = STARS_RE.sub(lambda m: f"<STARS_{m.group(1)}>", s)
        s = POS_RATE.sub("<RATING_POS>", s)
        s = NEG_RATE.sub("<RATING_NEG>", s)
    return " ".join(s.split())

def add_domain_tag(line: str, domain: str) -> str:
    return f"dom{domain} " + line

LEMMA_MAP = {
    # Adjectives (copula/colloquial/diacritic variants)
    "yax≈üƒ±dƒ±r": "yax≈üƒ±", "yax≈üƒ±di": "yax≈üƒ±", "yax≈üƒ±dƒ±": "yax≈üƒ±",
    "yaxsidir": "yax≈üƒ±", "yaxsidi": "yax≈üƒ±", "yaxshidir": "yax≈üƒ±",
    "pisdir": "pis", "pisdi": "pis",
    "g√∂z…ôldir": "g√∂z…ôl", "g√∂z…ôldi": "g√∂z…ôl", "gozeldir": "g√∂z…ôl", "gozeldi": "g√∂z…ôl", "gozel": "g√∂z…ôl",
    "bahalƒ±dƒ±r": "bahalƒ±", "bahalƒ±di": "bahalƒ±", "bahalƒ±dƒ±": "bahalƒ±",
    "bahalidir": "bahalƒ±", "bahalidi": "bahalƒ±", "bahali": "bahalƒ±",
    "ucuzdur": "ucuz", "ucuzdu": "ucuz",
    "m√ºk…ômm…ôldir": "m√ºk…ômm…ôl", "m√ºk…ômm…ôldi": "m√ºk…ômm…ôl", "mukemmeldir": "m√ºk…ômm…ôl", "mukemmel": "m√ºk…ômm…ôl",
    "…ôladƒ±r": "…ôla", "…ôladi": "…ôla", "eladir": "…ôla", "ela": "…ôla",
    "maraqlƒ±dƒ±r": "maraqlƒ±", "maraqlƒ±dƒ±": "maraqlƒ±", "maraqlidir": "maraqlƒ±",
    "z…ôifdir": "z…ôif", "z…ôifdi": "z…ôif", "zeifdir": "z…ôif",
    "√ß…ôtindir": "√ß…ôtin", "√ß…ôtindi": "√ß…ôtin", "cetindir": "√ß…ôtin",
    "asandƒ±r": "asan", "asandƒ±": "asan", "asand": "asan",
    "s√ºr…ôtlidir": "s√ºr…ôtli", "suretlidir": "s√ºr…ôtli",
    "keyfiyy…ôtlidir": "keyfiyy…ôtli", "keyfiyyetlidir": "keyfiyy…ôtli",
    "t…ômizdir": "t…ômiz", "t…ômizdi": "t…ômiz", "temizdir": "t…ômiz",
    "√ßirklidir": "√ßirkli", "√ßirkli": "√ßirkli",
    "rahatdƒ±r": "rahat", "rahatdi": "rahat", "rahat": "rahat",
    "narazƒ±dƒ±r": "narazƒ±", "memnundur": "m…ômnun", "m…ômnundur": "m…ômnun", "m…ômnunum": "m…ômnun", "memnunam": "m…ômnun",
    "q…ô≈ü…ôngdir": "q…ô≈ü…ông", "q…ô≈ü…ôngdi": "q…ô≈ü…ông", "qeseng": "q…ô≈ü…ông",
    "m√∂ht…ô≈ü…ômdir": "m√∂ht…ô≈ü…ôm", "mohtesemdir": "m√∂ht…ô≈ü…ôm", "mohtesem": "m√∂ht…ô≈ü…ôm",
    "d…ôh≈ü…ôtdir": "d…ôh≈ü…ôt", "dehsetdir": "d…ôh≈ü…ôt",
    "b…ôrbaddƒ±r": "b…ôrbad", "berbaddir": "b…ôrbad",
    "uyƒüundur": "uyƒüun", "uyƒüundu": "uyƒüun", "uygun": "uyƒüun",

    # Common nouns
    "xidm…ôtdir": "xidm…ôt", "xidmetdir": "xidm…ôt",
    "keyfiyy…ôtdir": "keyfiyy…ôt", "keyfiyyetdir": "keyfiyy…ôt",

    # Verbs ‚Üí simple stems
    "b…ôy…ônir…ôm": "b…ôy…ôn", "b…ôy…ôndim": "ÿ®…ôy…ôn", "b…ôy…ônmi≈ü…ôm": "b…ôy…ôn", "b…ôy…ônirik": "b…ôy…ôn",
    "sevir…ôm": "sev", "sevdim": "sev", "sevdik": "sev", "sevilir": "sev",
    "ist…ômir…ôm": "ist…ôm…ô", "ist…ôyir…ôm": "ist…ô", "isteyirem": "ist…ô",
    "aldƒ±m": "al", "aldƒ±q": "al", "alƒ±rƒ±q": "al", "aldƒ±lar": "al",
    "√∂d…ôdim": "√∂d…ô", "√∂d…ôdik": "√∂d…ô", "odenildi": "√∂d…ô",
    "g√∂rd√ºm": "g√∂r", "g√∂rd√ºk": "g√∂r", "g√∂r√ºr…ôm": "g√∂r",
    "yazdƒ±m": "yaz", "yazmƒ±≈üam": "yaz", "yazƒ±lƒ±r": "yaz",
    "oxudum": "oxu", "oxuyuram": "oxu",
    "i≈ül…ôyir": "i≈ül…ô", "i≈ül…ômir": "i≈ül…ô", "i≈ül…ôyir…ôm": "i≈ül…ô", "i≈ül…ôdi": "i≈ül…ô", "i≈ül…ômirdi": "i≈ül…ô",
    "√ßatdƒ±": "√ßat", "√ßatmadƒ±": "√ßat", "√ßatdƒ±rƒ±ldƒ±": "√ßat",
    "gecikdi": "gecik", "gecikir": "gecik",
    "g…ôldi": "g…ôl", "g…ôlir": "g…ôl",
    "getdim": "get", "getdi": "get", "gedir…ôm": "get", "gedir": "get",

    # Sentiment drift
    "yax≈üƒ±la≈üƒ±b": "yax≈üƒ±", "pisl…ô≈üib": "pis",
    "superdir": "super", "superdi": "super", "super": "super",
}


_COPULA_RE = re.compile(r"(d[ƒ±iu√º]r|di|dƒ±|du|d√º)$", flags=re.UNICODE)


LEMMA_CHANGED = 0

def simple_lemma_token(t: str) -> str:
    """Return lemma using LEMMA_MAP or by stripping copula if safe."""
    global LEMMA_CHANGED
    t0 = t
    if t in LEMMA_MAP:
        if LEMMA_MAP[t] != t:
            LEMMA_CHANGED += 1
        return LEMMA_MAP[t]
    stem = _COPULA_RE.sub("", t)
    if stem != t and len(stem) >= 3:
        LEMMA_CHANGED += 1
        return stem
    return t0

def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:
    if not isinstance(s, str): return ""
    for emo, tag in EMO_MAP.items():
        s = s.replace(emo, f" {tag} ")
    s = fix_text(s)
    s = html.unescape(s)
    s = HTML_TAG_RE.sub(" ", s)
    s = URL_RE.sub(" URL ", s)
    s = EMAIL_RE.sub(" EMAIL ", s)
    s = PHONE_RE.sub(" PHONE ", s)
    s = re.sub(r"#([A-Za-z0-9_]+)", lambda m: " " + re.sub(r"([a-z])([A-Z])", r"\1 \2", m.group(1)) + " ", s)
    s = USER_RE.sub(" USER ", s)
    s = lower_az(s)
    s = MULTI_PUNCT.sub(r"\1", s)
    if numbers_to_token:
        s = re.sub(r"\d+", " <NUM> ", s)
    if keep_sentence_punct:
        s = re.sub(r"[^\w\s<>'…ôƒüƒ±√∂√º√ß≈ü∆èƒûIƒ∞√ñ√ú√á≈ûxqXQ.!?]", " ", s)
    else:
        s = re.sub(r"[^\w\s<>'…ôƒüƒ±√∂√º√ß≈ü∆èƒûIƒ∞√ñ√ú√á≈ûxqXQ]", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    toks = TOKEN_RE.findall(s)
    norm = []
    mark_neg = 0
    for t in toks:
        t = REPEAT_CHARS.sub(r"\1\1", t)
        t = SLANG_MAP.get(t, t)
        if t in NEGATORS:
            norm.append(t)
            mark_neg = 3
            continue
        if mark_neg > 0 and t not in {"URL","EMAIL","PHONE","USER"}:
            norm.append(t + "_NEG")
            mark_neg -= 1
        else:
            t = simple_lemma_token(t)
            norm.append(t)
    norm = [t for t in norm if not (len(t) == 1 and t not in {"o","e"})]
    return " ".join(norm).strip()

def map_sentiment_value(v, scheme: str):
    if scheme == "binary":
        try: return 1.0 if int(v) == 1 else 0.0
        except Exception: return None
    s = str(v).strip().lower()
    if s in {"pos","positive","1","m√ºsb…ôt","good","pozitiv"}: return 1.0
    if s in {"neu","neutral","2","neytral"}: return 0.5
    if s in {"neg","negative","0","m…ônfi","bad","neqativ"}: return 0.0
    return None

def process_file(in_path, text_col, label_col, scheme, out_two_col_path, remove_stopwords=False):
    df = pd.read_excel(in_path)
    for c in ["Unnamed: 0","index"]:
        if c in df.columns: df = df.drop(columns=[c])
    assert text_col in df.columns and label_col in df.columns, f"Missing columns in {in_path}"

    df = df.dropna(subset=[text_col])
    df = df[df[text_col].astype(str).str.strip().str.len() > 0]
    df = df.drop_duplicates(subset=[text_col])
    DEASCII_RE = re.compile(r"\b(cox|yaxsi)\b")
    pre_lower = df[text_col].astype(str).map(lower_az)
    deasciify_changed = int(pre_lower.str.findall(DEASCII_RE).str.len().sum())

    df["cleaned_text"] = df[text_col].astype(str).apply(normalize_text_az)
    df["__domain__"]   = df[text_col].astype(str).apply(detect_domain)
    df["cleaned_text"] = df.apply(lambda r: domain_specific_normalize(r["cleaned_text"], r["__domain__"]), axis=1)

    if remove_stopwords:
        sw = set(["v…ô","il…ô","amma","ancaq","lakin","ya","h…ôm","ki","bu","bir","o","biz","siz","m…ôn","s…ôn",
                  "orada","burada","b√ºt√ºn","h…ôr","artƒ±q","√ßox","az","…ôn","d…ô","da","√º√ß√ºn"])
        for keep in ["deyil","yox","he√ß","q…ôtiyy…ôn","yoxdur"]:
            sw.discard(keep)
        df["cleaned_text"] = df["cleaned_text"].apply(lambda s: " ".join([t for t in s.split() if t not in sw]))

    df["sentiment_value"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))
    df = df.dropna(subset=["sentiment_value"])
    df["sentiment_value"] = df["sentiment_value"].astype(float)

    out_df = df[["cleaned_text","sentiment_value"]].reset_index(drop=True)
    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)
    out_df.to_excel(out_two_col_path, index=False)
    print(f"Saved: {out_two_col_path} (rows={len(out_df)})")
    print(f"[deasciify] changed_tokens={deasciify_changed}")
    print(f"[Lemma] tokens changed (rule-based): {LEMMA_CHANGED}")

def build_corpus_txt(input_files, text_cols, out_txt="corpus_all.txt"):
    lines = []
    for (f, text_col) in zip(input_files, text_cols):
        df = pd.read_excel(f)
        for raw in df[text_col].dropna().astype(str):
            dom = detect_domain(raw)
            s = normalize_text_az(raw, keep_sentence_punct=True)
            parts = re.split(r"[.!?]+", s)
            for p in parts:
                p = p.strip()
                if not p: continue
                p = re.sub(r"[^\w\s…ôƒüƒ±√∂√º√ß≈ü∆èƒûIƒ∞√ñ√ú√á≈ûxqXQ]", " ", p)
                p = " ".join(p.split()).lower()
                if p:
                    lines.append(f"dom{dom} " + p)
    with open(out_txt, "w", encoding="utf-8") as w:
        for ln in lines:
            w.write(ln + "\n")
    print(f"Wrote {out_txt} with {len(lines)} lines")

if __name__ == "__main__":
    CFG = [
        ("labeled-sentiment.xlsx",      "text", "sentiment", "tri"),
        ("test__1_.xlsx",               "text", "label",     "binary"),
        ("train__3_.xlsx",              "text", "label",     "binary"),
        ("train-00000-of-00001.xlsx",   "text", "labels",    "tri"),
        ("merged_dataset_CSV__1_.xlsx", "text", "labels",    "binary"),
    ]
    for fname, tcol, lcol, scheme in CFG:
        out = f"{Path(fname).stem}_2col.xlsx"
        process_file(fname, tcol, lcol, scheme, out, remove_stopwords=False)
    build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG], out_txt="corpus_all.txt")
