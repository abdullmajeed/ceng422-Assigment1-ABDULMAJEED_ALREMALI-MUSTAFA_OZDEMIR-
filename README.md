1) Data & Goal
This project used five Azerbaijani text datasets provided in the assignment:
labeled-sentiment.xlsx, test__1__.xlsx, train__3__.xlsx, train-00000-of-00001.xlsx, and merged_dataset_CSV__1__.xlsx.
Each dataset was converted into a two-column Excel file containing cleaned_text and its sentiment_value (0.0 = Negative, 0.5 = Neutral, 1.0 = Positive).
Neutral samples (0.5) were intentionally kept to preserve a realistic distribution of sentiments.
Removing them would bias the embeddings toward extreme polarities and reduce the modelâ€™s ability to represent subtle or mixed emotions.
2) Preprocessing (rules & examples)
We applied Azerbaijani-aware, rule-based cleaning to all datasets before labeling:
â€¢	Unicode & casing: fix_text + html.unescape, NFC normalization; Azerbaijani lowercase (Ä°â†’i, Iâ†’Ä±).
â€¢	Entity masking: URLsâ†’URL, emailsâ†’EMAIL, phone numbersâ†’PHONE, mentionsâ†’USER.
â€¢	HTML/markup: strip <tags> and entities.
â€¢	Hashtags: drop # but keep the text and split camelCase (e.g., #QarabagIsBack â†’ qarabag Is Back).
â€¢	Digits: numbers â†’ <NUM>.
â€¢	Repeated letters: collapse â‰¥3 to 2 (e.g., cooool â†’ coool).
â€¢	Punctuation & spaces: collapse multi-punctuation and extra spaces; keep Azerbaijani letters É™, ÄŸ, Ä±, Ã¶, Ã¼, Ã§, ÅŸ.
â€¢	Single-letter tokens: remove except o, e.
â€¢	Row hygiene: drop exact duplicates and rows that become empty after cleaning.
Before â†’ After (samples)
â€¢	Bu **film** Ã§ox yaxÅŸÄ±dÄ±r!!! ğŸ˜‚ â†’ bu film Ã§ox yaxÅŸÄ±dÄ±r EMO_POS
â€¢	QiymÉ™t: 35 azn, É™laqÉ™: +994â€¦ â†’ qiymÉ™t <NUM> azn É™laqÉ™ PHONE
â€¢	#QarabagIsBack!! â†’ qarabag is back
â€¢	<a href="â€¦">link</a> yazmÄ±ÅŸdÄ±m â†’ link yazmÄ±ÅŸdÄ±m
After cleaning, each file was saved as a two-column Excel with cleaned_text and sentiment_value.
3) Mini Challenges
We implemented all five mini challenges required by the assignment:
â€¢	Hashtag split: Automatically split camelCase hashtags (e.g., #QarabagIsBack â†’ qarabag is back) while removing the # symbol.
â€¢	Emoji mapping: A small emoji dictionary (EMO_MAP) mapped positive emojis to <EMO_POS> and negative ones to <EMO_NEG> before tokenization.
â€¢	Stopword research: Compared Azerbaijani with Turkish stopwords and defined a small optional list (e.g., â€œvÉ™â€, â€œilÉ™â€, â€œammaâ€, â€œlakinâ€, â€œyaâ€, â€œhÉ™mâ€, â€œkiâ€, â€œbuâ€, â€œbirâ€, â€œoâ€). Negations (â€œyoxâ€, â€œdeyilâ€, â€œheÃ§â€, â€œqÉ™tiyyÉ™nâ€, â€œyoxdurâ€) were intentionally preserved. The option --remove-stopwords can toggle this behavior (default: off).
â€¢	Negation scope: After each negation, the next three tokens were marked with _NEG (e.g., heÃ§ bir nÉ™ticÉ™ vermÉ™di â†’ heÃ§_NEG bir_NEG nÉ™ticÉ™_NEG vermÉ™di), improving contextual polarity handling.
â€¢	Simple deasciify: Applied a small map (coxâ†’Ã§ox, yaxsiâ†’yaxÅŸÄ±) and reported token changes. Across all datasets, 2,187 tokens were corrected during deasciification.
4) Domain-Aware
We implemented automatic domain detection and domain-specific normalization to make the corpus context-aware.
â€¢	Detection rules:
Used keyword-based hints to classify each text into one of three domains:
o	News â†’ keywords like â€œapaâ€, â€œtrendâ€, â€œazertacâ€, â€œreutersâ€.
o	Social â†’ includes â€œ@â€, â€œ#â€, or emojis (ğŸ˜‚ğŸ˜ğŸ™‚ğŸ˜¡).
o	Reviews â†’ contains â€œaznâ€, â€œqiymÉ™tâ€, â€œulduzâ€, or â€œÃ§ox yaxÅŸÄ±â€.
Texts that did not match any pattern were labeled as general.
â€¢	Domain normalization (for reviews):
Replaced sentiment and rating phrases with special tags:
price â†’ <PRICE>, 1â€“5 stars â†’ <STARS_#>,
Ã§ox yaxÅŸÄ± â†’ <RATING_POS>, Ã§ox pis â†’ <RATING_NEG>.
â€¢	Domain tags:
During corpus generation, each line in corpus_all.txt was automatically prefixed with its detected domain label using the pattern:
dom<domain> <sentence> (e.g., domreviews, domsocial, domnews, domgeneral).
This produced a domain-tagged corpus with 124,353 sentences for model training.
5) Embeddings: Training Settings and Results
Two embedding models â€” Word2Vec and FastText â€” were trained on the combined, cleaned, and domain-tagged corpus.
Training Settings
Parameter	Word2Vec	FastText
Architecture	Skip-gram (sg=1)	Skip-gram (sg=1)
Vector size	300	300
Window	5	5
Min count	3	3
Negative samples	10	10
Epochs	10	10
Subword n-grams	â€”	3â€“6
Both models were trained using gensim and saved as:
â€¢	embeddings/word2vec.model
â€¢	embeddings/fasttext.model
Evaluation Results
Metric	Word2Vec	FastText
Vocabulary Coverage (avg)	0.960	0.960
Synonym Similarity (avg)	0.312	0.417
Antonym Similarity (avg)	0.306	0.421
Separation (Synâ€“Ant)	+0.006	â€“0.004
Nearest Neighbors (Samples)
Word	Word2Vec Neighbors	FastText Neighbors
yaxÅŸÄ±	yaxshi, iyi, <RATING_POS>	yaxÅŸÄ±1, yaxÅŸÄ±ca, yaxÅŸÄ±ya
pis	<RATING_NEG>, gÃ¼nd, lire	piis, pisdii, pi
Ã§ox	Ã§oxx, gÃ¶zÉ™ldir	Ã§oxx, Ã§oh
bahalÄ±	portretlÉ™rinÉ™, villalarÄ±	bahallÄ±, baha1sÄ±, pahalÄ±
ucuz	ÅŸeytanbazardan, dÃ¼zÉ™ldirilib	ucuzu, ucuzca, ucuzdu
Interpretation
â€¢	FastText achieved higher synonym and antonym similarity, confirming that its subword-based approach captures better semantic and morphological relations, especially in rich and noisy Azerbaijani text.
â€¢	Word2Vec performed competitively but was slightly weaker in handling rare or misspelled forms.
â€¢	Both embeddings provide meaningful nearest neighbors, validating the overall corpus and preprocessing pipeline.
7) Reproducibility
The project is fully reproducible and organized into three modular scripts:
Stage	Script	Description
Preprocessing	 preprocess_and_corpus.py	Cleans all five datasets, removes duplicates, exports 2-column Excel files, and builds the domain-tagged corpus corpus_all.txt.
 Training	train_embeddings.py	Trains Word2Vec and FastText models using the cleaned corpus and saves them in embeddings/.
Evaluation	compare_embeddings.py	Compares both models using coverage, synonym/antonym similarity, and nearest-neighbor quality metrics.
Environment
â€¢	Python 3.9 or higher (tested on Windows 10)
â€¢	Required packages:
â€¢	pip install -U pandas gensim openpyxl ftfy regex scikit-learn
Deterministic Results
To ensure identical results across runs:
1.	Disable Pythonâ€™s hash randomization:
2.	set PYTHONHASHSEED=0        # Windows  
3.	export PYTHONHASHSEED=0     # Linux / macOS
4.	Use a fixed random seed during training:
5.	seed = 42
6.	Word2Vec(..., seed=seed)
7.	FastText(..., seed=seed)
How to run (end-to-end):
1.	Put the 5 raw Excel files in the project root directory.
2.	Preprocess + export 2-column files + build corpus
3.	python 1_preprocess_and_corpus.py
Outputs:
o	*_2col.xlsx (cleaned sentiment files)
o	corpus_all.txt (domain-tagged corpus)
4.	Train embeddings (Word2Vec & FastText)
5.	python 2_train_embeddings.py
Outputs:
o	embeddings/word2vec.model
o	embeddings/fasttext.model
6.	Evaluate & compare models
7.	python 3_compare_embeddings.py
Outputs:
o	Printed coverage, synonym/antonym similarity, nearest neighbors
8.	Notes:
o	All runs are CPU-friendly (no GPU required).
o	Results are reproducible on repeated runs using the same random seed (seed = 42) and hash control (PYTHONHASHSEED = 0).
8) Conclusions
This assignment successfully demonstrated the complete Azerbaijani sentiment analysis pipeline, covering data cleaning, domain detection, corpus construction, and embedding training with Word2Vec and FastText.
â€¢	The preprocessing pipeline effectively normalized noisy Azerbaijani text, handled negations, emojis, hashtags, and domain-specific patterns.
â€¢	The domain-aware corpus (corpus_all.txt) enabled richer contextual learning across news, social, and review text types.
â€¢	Both embeddings produced meaningful semantic structures; however, FastText achieved higher coverage and better synonym similarity due to its subword modeling.
â€¢	The evaluation confirmed that FastText handles morphological variations and rare words more effectively, while Word2Vec remains competitive for frequent vocabulary.
â€¢	All experiments were reproducible and CPU-efficient, meeting the projectâ€™s reproducibility and interpretability criteria.
Future work: integrating a more advanced Azerbaijani lemmatizer, expanding domain coverage, and exploring transformer-based embeddings such as BERT for deeper contextual understanding.
### Group Members
- Abdulmajeed Alremali (22050941025)
